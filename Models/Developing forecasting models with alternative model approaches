
Exponential Smoothing                                                    

Exponential Smoothing uses past values to calculate a forecast. The strength with which each value influences the forecast is weakened with help of a 
smoothing parameter. Thus we are dealing with a weighted average, whose values fade out the longer ago they were in the past.


  Simple expontential smoothing                                          

  Formula: ses(). It must be decided if alpha (the smoothing parameter should be automatically calculated. If initial=simple, the alpha value can 
  be set to any chosen value, if initial=optimal (or nothing, as this is the default), alpha will be set to the optimal value based on ets().
  h=12 gives the number of cycles for the forecast.

  Model_ses <- ses(TotalAsIs, h=12)
  summary(Model_ses)
  plot(Model_ses)

  The Akaike's Information Criterion(AIC/AICc) or the Bayesian Information 
  Criterion (BIC) should be at minimum.

  plot(Model_ses, plot.conf=FALSE, ylab="Exports Chulwalar  )", xlab="Year", main="", fcol="white", type="o")
  lines(fitted(Model_ses), col="green", type="o")
  lines(Model_ses$mean, col="blue", type="o")
  legend("topleft",lty=1, col=c(1,"green"), c("data", expression(alpha == 0.671)),pch=1)



  Holt's linear trend method                                             

  Holt added to the model in order to forecast using trends as well. For this it is necessary to add a beta, which determines the trend.
  If neither alpha nor beta is stated, both parameters will be optimised using ets(). 

  Model_holt_1 <- holt(TotalAsIs,h=12)
  summary(Model_holt_1)
  plot(Model_holt_1)

The trend is exponential if the intercepts(level) and the gradient (slope) are multiplied with eachother. The values are worse. As the Beta was very low in 
the optimisation, the forecast is very similar to the ses() model. 


  Model_holt_2<- holt(TotalAsIs, exponential=TRUE,h=12)
  summary(Model_holt_2)
  plot(Model_holt_2)

As such simple trends tend to forecast the future to positively, we have added a dampener. Similar values to that of Model_holt_1 

  Model_holt_3 <- holt(TotalAsIs, damped=TRUE,h=12)
  summary(Model_holt_3)
  plot(Model_holt_3)

This also works for exponential trends.  The values remain worse. 

  Model_holt_4 <- holt(TotalAsIs, exponential=TRUE, damped=TRUE,h=12)
  summary(Model_holt_4)
  plot(Model_holt_4)

level and slope can be plotted individually for each model. 
  plot(Model_holt_1$model$state)
  plot(Model_holt_2$model$state)
  plot(Model_holt_3$model$state)
  plot(Model_holt_4$model$state)

  plot(Model_holt_1, plot.conf=FALSE, ylab="Exports Chulwalar  )", xlab="Year", main="", fcol="white", type="o")
  lines(fitted(Model_ses), col="purple", type="o")
  lines(fitted(Model_holt_1), col="blue", type="o")
  lines(fitted(Model_holt_2), col="red", type="o")
  lines(fitted(Model_holt_3), col="green", type="o")
  lines(fitted(Model_holt_4), col="orange", type="o")
  lines(Model_ses$mean, col="purple", type="o")
  lines(Model_holt_1$mean, col="blue", type="o")
  lines(Model_holt_2$mean, col="red", type="o")
  lines(Model_holt_3$mean, col="green", type="o")
  lines(Model_holt_4$mean, col="orange", type="o")
  legend("topleft",lty=1, col=c(1,"purple","blue","red","green","orange"), c("data", "SES","Holts auto", "Exponential", "Additive Damped", "Multiplicative Damped"),pch=1)

As these forecasts are not very convincing at the moment, there is no needto export the data.


Holt-Winter's seasonal method                                          


Holt and Winters have expanded Holt's model further to include the seasonality aspect. The parameter gamma, which is for smoothing the
seasonality, was added to achieve this. The values are better than the models without seasonality. This logical matches our results from the regression approaches, 
the data is strongly influenced by seasonality.  In the following model, none of the parameters are given so that they
will be optimised automatically. There are two models: one using an additive error model method and one using a multiplicative error model.

  Model_hw_1 <- hw(TotalAsIs ,seasonal="additive",h=12)
  summary(Model_hw_1)
  plot(Model_hw_1)
 

  Model_hw_2 <- hw(TotalAsIs ,seasonal="multiplicative",h=12)
  summary(Model_hw_2)
  plot(Model_hw_2)


The additive model gives slightly better results than the multiplicative model.

  plot(Model_hw_1, ylab="Exports Chulwalar  ", plot.conf=FALSE, type="o", fcol="white", xlab="Year")
  lines(fitted(Model_hw_1), col="red", lty=2)
  lines(fitted(Model_hw_2), col="green", lty=2)
  lines(Model_hw_1$mean, type="o", col="red")
  lines(Model_hw_2$mean, type="o", col="green")
  legend("topleft",lty=1, pch=1, col=1:3, c("data","Holt Winters' Additive","Holt Winters' Multiplicative"))

In order to use the results later, they need to be converted into point forcasts.

  Model_hw_1_df <-as.data.frame(Model_hw_1) 
  Model_hw_1_PointForecast <- ts(Model_hw_1_df$"Point Forecast", start=c(2014,1), end=c(2014,12), frequency=12)
  Model_hw_1_PointForecast
  Model_hw_2_df <-as.data.frame(Model_hw_2) 
  Model_hw_2_PointForecast <- ts(Model_hw_2_df$"Point Forecast", start=c(2014,1), end=c(2014,12), frequency=12)
  Model_hw_2_PointForecast


Innovations state space models for exponential smoothing               

The funktion ets() produces a model with the same values as Model_hw_1.  The reason for this is that all of the parameters in this model were optimised 
using the ets() function. The results are a ets(A,A,A) model which is an additive method for trend, seasonality and errors. The previous models
also showed the type of ets() model in their summary. In this case the user parameters were either accepted or rejected. As the model has been set to 
"ZZZ", the best model will be automatically chosen. 

  Model_ets <-ets(TotalAsIs, model="ZZZ", damped=NULL, alpha=NULL, beta=NULL, gamma=NULL, phi=NULL, additive.only=FALSE, lambda=NULL, lower=c(rep(0.0001,3), 0.8), upper=c(rep(0.9999,3),0.98), opt.crit=c("lik","amse","mse","sigma","mae"), nmse=3, bounds=c("both","usual","admissible"), ic=c("aicc","aic","bic"), restrict=TRUE)
  summary(Model_ets)

  plot(Model_ets)
  Model_ets_forecast <- forecast(Model_ets,h=12)
  plot(Model_ets_forecast)


In order to use the results later, they need to be converted into point forcasts.
  Model_ets_forecast_df <-as.data.frame(Model_ets_forecast) 
  Model_ets_PointForecast <- ts(Model_ets_forecast_df$"Point Forecast", start=c(2014,1), end=c(2014,12), frequency=12)
  Model_ets_PointForecast



ARIMA       							                                                


AR = Autoregression
A Regression of a variable with itself. The autoregressive model specifies that the output variable depends linearly on its own previous values.
MA = Moving Average
The rolling average of past forecast errors. This model should not be confused with moving average smoothing, which is used
for establishing trends and is based on past values. 

ARIMA = AutoRegressive Integrated Moving Average model
A combination of Differencing, Autoregression and Moving Average. Integration is the opposite of differencing.

Differencing
In order to make the time series stationary, it is necessary to difference. The data should be already stationary. This was tested using the Augmented Dickey-Fuller Test
adf.test(TotalEtelAsIs, alternative = "stationary")

  The p-value is less than 0,05. This means that the data is stationary, as the 0-Hypothesis of the test is "The data are not stationary".

Another possibility is the Kwiatkowski-Phillips-Schmidt-Shin Test
kpss.test(TotalEtelAsIs)

This test swaps the hypothesis so that a low p-value means that it is necessary to difference. The p-value here is under 0,01 and a warning is shown.

As the test failed to deliver a clear result, the data was then differenced and then retested. 

ChulwalarDiff <- diff(TotalEtelAsIs)

adf.test(ChulwalarDiff, alternative = "stationary")
kpss.test(ChulwalarDiff)

The kpss.test now has a p-value of more than 0,1, which hints that the data is stationary. 

tsdisplay(ChulwalarDiff)
However this plot shows that the months correlate stongly with the values from the previous year. This plot shows a  ACF
(autocorrelation function) and a PACF (partial autocorrelation function).

The following is a test method to distinguish the number of "normal" differencing rounds and seasonal differencing rounds. 
Seasonal differencing is used for data which is dominated by seasonality. The time series has been assigned a lag.

ns <- nsdiffs(TotalEtelAsIs)
if(ns > 0) {
  xstar <- diff(TotalEtelAsIs,lag=frequency(TotalEtelAsIs),differences=ns)
} else {
  xstar <- TotalEtelAsIs
}
nd <- ndiffs(xstar)
if(nd > 0) {
  xstar <- diff(xstar,differences=nd)
}

nd # Number of normal differencing rounds
ns # Number of seasonal differencing rounds
xstar # The output "xstar" has been differenced correctly. 
tsdisplay(xstar)

# If "lag" is set to 12, this is equivalent to 1* seasonal differencing
ChulwalarDiff_lag <- diff(TotalEtelAsIs,lag=12)

adf.test(ChulwalarDiff_lag, alternative = "stationary")
kpss.test(ChulwalarDiff_lag)

tsdisplay(ChulwalarDiff)
tsdisplay(ChulwalarDiff_lag)

plot(ChulwalarDiff)
lines(ChulwalarDiff_lag, col="red") 
# The time series appears much more "stationary".


ARIMA modelling						                                            


The values for AIC, AICc and BIC should be minimized.
So a range of combinations was tested.


Model_ARIMA_1  <- Arima(TotalEtelAsIs, order=c(0,1,0))
summary(Model_ARIMA_1 )
plot(forecast(Model_ARIMA_1 ))


Model_ARIMA_2 <- Arima(TotalEtelAsIs, order=c(1,1,0))
summary(Model_ARIMA_2)
plot(forecast(Model_ARIMA_2))


Model_ARIMA_3 <- Arima(TotalEtelAsIs, order=c(1,1,1))
summary(Model_ARIMA_3)
plot(forecast(Model_ARIMA_3))


Model_ARIMA_4 <- Arima(TotalEtelAsIs, order=c(2,1,1))
summary(Model_ARIMA_4)
plot(forecast(Model_ARIMA_4))


Model_ARIMA_5 <- Arima(TotalEtelAsIs, order=c(2,1,2))
summary(Model_ARIMA_5)
plot(forecast(Model_ARIMA_5))


Model_ARIMA_6 <- Arima(TotalEtelAsIs, order=c(3,1,2))
summary(Model_ARIMA_6)
plot(forecast(Model_ARIMA_6))


Model_ARIMA_7 <- Arima(TotalEtelAsIs, order=c(3,1,3))
summary(Model_ARIMA_7)
plot(forecast(Model_ARIMA_7))


Model_ARIMA_8 <- Arima(TotalEtelAsIs, order=c(3,1,1))
summary(Model_ARIMA_8)
plot(forecast(Model_ARIMA_8))


Model_ARIMA_9 <- Arima(TotalEtelAsIs, order=c(3,1,2))
summary(Model_ARIMA_9)
plot(forecast(Model_ARIMA_9))


Model_ARIMA_10 <- Arima(TotalEtelAsIs, order=c(1,1,3))
summary(Model_ARIMA_10)
plot(forecast(Model_ARIMA_10))


Model_ARIMA_11 <- Arima(TotalEtelAsIs, order=c(2,1,3))
summary(Model_ARIMA_11)
plot(forecast(Model_ARIMA_11))


Model_ARIMA_12 <- Arima(TotalEtelAsIs, order=c(2,2,3))
summary(Model_ARIMA_12)
plot(forecast(Model_ARIMA_12))


Model_ARIMA_13 <- Arima(TotalEtelAsIs, order=c(2,3,2))
summary(Model_ARIMA_13)
plot(forecast(Model_ARIMA_13))


Acf(residuals(Model_ARIMA_13))
Box.test(residuals(Model_ARIMA_13), lag=12, fitdf=4, type="Ljung")
The Ljung-Box Test has H0: The data are independently distributed 
und Ha: The data are not independently distributed. 

Just like the remainder showed before, there is a definite coherence


Seasonal ARIMA modelling   				                                    

This model integrates the seasonal aspect into the ARIMA model. As the previous models all had a peak in lag 12, it seems viable that the data are seasonal. 

Model_Seasonal_ARIMA_0 <- Arima(TotalEtelAsIs, order=c(0,0,0), seasonal=c(1,0,0))
tsdisplay(residuals(Model_Seasonal_ARIMA_0))
summary(Model_Seasonal_ARIMA_0)


Model_Seasonal_ARIMA_1 <- Arima(TotalEtelAsIs, order=c(0,1,1), seasonal=c(0,1,1))
summary(Model_Seasonal_ARIMA_1)
plot(forecast(Model_Seasonal_ARIMA_1))


# Insert the values from the previous chapter for the non-seasonal values. 
Model_Seasonal_ARIMA_2 <- Arima(TotalEtelAsIs, order=c(2,3,2), seasonal=c(1,1,1))
tsdisplay(residuals(Model_Seasonal_ARIMA_2))
summary(Model_Seasonal_ARIMA_2)
plot(forecast(Model_Seasonal_ARIMA_2))


# Good results when using drift.
Model_Seasonal_ARIMA_3 <- Arima(TotalEtelAsIs, order=c(1,0,1), seasonal=c(1,1,1),include.drift=TRUE)
tsdisplay(residuals(Model_Seasonal_ARIMA_3))
summary(Model_Seasonal_ARIMA_3)
plot(forecast(Model_Seasonal_ARIMA_3))


Model_Seasonal_ARIMA_4 <- Arima(TotalEtelAsIs, order=c(2,3,2), seasonal=c(1,3,2))
tsdisplay(residuals(Model_Seasonal_ARIMA_4))
summary(Model_Seasonal_ARIMA_4)
plot(forecast(Model_Seasonal_ARIMA_4))



Model_Seasonal_ARIMA_5 <- Arima(TotalEtelAsIs, order=c(2,3,2), seasonal=c(1,4,2))
tsdisplay(residuals(Model_Seasonal_ARIMA_5))
summary(Model_Seasonal_ARIMA_5)
plot(forecast(Model_Seasonal_ARIMA_5))


The more the seasonal aspect is changed, the better the results based on AIC, AICc and BIC. Theoretically the models should more and more suitable for the forecast.
However, a look at the plot of the forecasts shows that the changes are making the  data less and less convincing and thus unuseable. 


Auto-ARIMA modelling						                                        

The automatic establishment of an ARIMA model shows that (2,0,1)(0,1,1) with drift delivers the best results. 



Model_auto.arima <- auto.arima(TotalEtelAsIs)
summary(Model_auto.arima)
CV(ModelWithTrendAndSeasonalityOnly)

Acf(residuals(Model_auto.arima))
Box.test(residuals(Model_auto.arima), lag=12, fitdf=4, type="Ljung")
The Ljung-Box Test has H0: The data are independently distributed and Ha: The data are not independently distributed. The results show: White noise

Model_auto.arima_forecast <- forecast(Model_auto.arima,h=12)
plot(Model_auto.arima_forecast)
Model_auto.arima_forecast

Model_auto.arima_forecast_df <-as.data.frame(Model_auto.arima_forecast) 
Model_auto.arima_PointForecast <- ts(Model_auto.arima_forecast_df$"Point Forecast", start=c(2014,1), end=c(2014,12), frequency=12)
Model_auto.arima_PointForecast


 Dynamic regression models					                                      


Regression models are combined with ARIMA models on order to make sure that external factors are included and that the time series are not only forecasted 
based on past values. A regression of the ARIMA errors should be aspired for. 

The time series was differniated and the SIGov Indicator are not stationary. So that a forecast can be produced, the indicator has to be lagged
so that we have values for 2014. 

CEPI_lagged <- ts(c(rep(NA,12),CEPIVector),start=c(2008,1), end=c(2013,12), frequency=12)
CEPI_2014_lagged <- ts(CEPI_2013, start=c(2014,1), end=c(2014,12), frequency=12)

Model_dynreg <- Arima(TotalEtelAsIs, xreg=CEPI_lagged, order=c(2,2,0))
tsdisplay(arima.errors(Model_dynreg), main="ARIMA errors")
summary(Model_dynreg)

Model_dynreg_auto.arima <- auto.arima(TotalAsIs, xreg=CEPI_lagged)
summary(Model_dynreg_auto.arima)
tsdisplay(arima.errors(Model_dynreg_auto.arima), main="ARIMA errors")



Acf(residuals(Model_dynreg_auto.arima))
Box.test(residuals(Model_dynreg_auto.arima), lag=12, fitdf=4, type="Ljung")
The Ljung-Box Test has H0: The data are independently distributed and Ha: The data are not independently distributed. The results show: White noise

summary(Model_dynreg_auto.arima)
Model_dynreg_auto.arima_forecast <- forecast(Model_dynreg_auto.arima, xreg=CEPI_2014_lagged,h=12)
plot(Model_dynreg_auto.arima_forecast)
Model_dynreg_auto.arima_forecast

Model_dynreg_auto.arima_forecast_df <-as.data.frame(Model_dynreg_auto.arima_forecast) 
Model_dynreg_auto.arima_PointForecast <- ts(Model_dynreg_auto.arima_forecast_df$"Point Forecast", start=c(2014,1), end=c(2014,12), frequency=12)
Model_dynreg_auto.arima_PointForecast
